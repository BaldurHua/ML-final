{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import os\n",
    "import cv2 as cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        out_channels_branch = 16 \n",
    "        self.branch1x1 = nn.Conv2d(in_channels, out_channels_branch, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, out_channels_branch, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(out_channels_branch, out_channels_branch, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, out_channels_branch, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(out_channels_branch, out_channels_branch, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(out_channels_branch, out_channels_branch, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, out_channels_branch, kernel_size=1)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResInceptionNet(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(ResInceptionNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.res_block1 = ResBlock(32)\n",
    "        self.res_block2 = ResBlock(32)\n",
    "        \n",
    "        # Inception module\n",
    "        self.inception = InceptionModule(32)\n",
    "        \n",
    "        # Double convolution layers\n",
    "        self.conv2_1 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 2048)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        self.fc4 = nn.Linear(512, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.inception(x)\n",
    "        \n",
    "        x = self.relu2_1(self.bn2_1(self.conv2_1(x)))\n",
    "        x = self.relu2_2(self.bn2_2(self.conv2_2(x)))\n",
    "        x = self.relu3_1(self.bn3_1(self.conv3_1(x)))\n",
    "        x = self.relu3_2(self.bn3_2(self.conv3_2(x)))\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop1(F.relu(self.fc1(x)))\n",
    "        x = self.drop2(F.relu(self.fc2(x)))\n",
    "        x = self.drop3(F.relu(self.fc3(x)))  \n",
    "        x = self.fc4(x)  \n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = ResInceptionNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: 'angry', 1: 'disgusted', 2: 'fearful', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprised'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_tensor, model):\n",
    "    model.eval()  \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor = image_tensor.cuda()\n",
    "        model = model.cuda()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "        max_prob, preds = torch.max(probabilities, dim=1)\n",
    "        predicted_label = preds[0].item()\n",
    "        probability = max_prob[0].item()\n",
    "        emotion_text = classes[predicted_label] \n",
    "        return emotion_text, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5077, 0.5077, 0.5077], [0.2550, 0.2550, 0.2550])\n",
    "])\n",
    "\n",
    "\n",
    "model = ResInceptionNet() \n",
    "model.load_state_dict(torch.load('ResInceptionNet3.pth'))  \n",
    "\n",
    "# predict(video_path, model, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = os.path.join(os.getcwd(), 'Test Video.mp4')\n",
    "flag_live = False #True False\n",
    "flag_model = False #True False\n",
    "\n",
    "if flag_live == True:\n",
    "    # Initialize the camere\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"The camera is not available\")\n",
    "        exit()\n",
    "else:\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "    if not cap.isOpened():\n",
    "        print(\"The video is not available\")\n",
    "        exit()\n",
    "        \n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "save_video = False \n",
    "if save_video:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('output_video.avi', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    # Return a boolean if the frame is available and the image\n",
    "    ret, color_frame = cap.read()  \n",
    "    #print(color_frame)\n",
    "    if not ret:\n",
    "        print(\"The frame is not available\")\n",
    "        break\n",
    "    # Convert the captured image to grayscale\n",
    "    gray_frame = cv2.cvtColor(color_frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Call the OpenCV Face recognition\n",
    "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    face = face_classifier.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n",
    "\n",
    "    for (x, y, w, h) in face:\n",
    "        # Create a rectangule in the image\n",
    "        rect = cv2.rectangle(color_frame, (x, y), (x + w, y + h), color=(255, 185, 0), thickness=5)\n",
    "        # Capture the face in the rectangule (used to predict the emotion)\n",
    "        rect_gray = gray_frame[y:y + w, x:x + h]\n",
    "        rect_gray = cv2.resize(rect_gray, (48, 48))\n",
    "        face_pil = Image.fromarray(rect_gray)  # Convert numpy array to PIL Image\n",
    "\n",
    "        if face_pil.mode != 'L':\n",
    "            face_pil = face_pil.convert('L')\n",
    "\n",
    "        # Convert a single-channel image (L) directly to RGB\n",
    "        face_pil = face_pil.convert(\"RGB\")\n",
    "\n",
    "        face_tensor = transform(face_pil)\n",
    "        predicted_emotion, probability = predict(face_tensor.unsqueeze(0), model)\n",
    "        emotion_text = f\"{predicted_emotion}: {probability:.2f}\"\n",
    "        cv2.putText(color_frame, emotion_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "        \n",
    "    if save_video:\n",
    "        out.write(color_frame)   \n",
    "            \n",
    "    cv2.imshow('frame', color_frame)\n",
    "\n",
    "    # Include a quit botton (press q to quit)\n",
    "    if cv2.waitKey(20) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "if save_video:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "443827bf24667f421fb10725def0166fa1673cc655d9aeee845987250124cade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  \n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 256)  \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout(self.pool(F.relu(self.bn3(self.conv3(x)))))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'angry': 0, 'disgusted': 1, \n",
    "           'fearful': 2, 'happy': 3, \n",
    "           'neutral': 4, 'sad': 5, \n",
    "           'surprised': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path, model, transform):\n",
    "    model.eval() \n",
    "    with torch.no_grad():  \n",
    "        img = Image.open(img_path).convert('RGB') \n",
    "        img = transform(img) \n",
    "        img = img.unsqueeze(0) \n",
    "        if torch.cuda.is_available(): \n",
    "            img = img.cuda()  \n",
    "            model = model.cuda() \n",
    "        y = model(img)\n",
    "        _, preds = torch.max(y, dim=1)\n",
    "        predicted_label = preds[0].item()\n",
    "        return {v: k for k, v in classes}[predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((48, 48)),  \n",
    "    T.ToTensor(),  \n",
    "    T.Normalize([0.5077, 0.5077, 0.5077], [0.2550, 0.2550, 0.2550]) \n",
    "])\n",
    "\n",
    "\n",
    "model = CNN2()  \n",
    "model.load_state_dict(torch.load('CNN2.pth'))  \n",
    "model.eval()\n",
    "\n",
    "# predict(video_path, model, transform)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "443827bf24667f421fb10725def0166fa1673cc655d9aeee845987250124cade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
